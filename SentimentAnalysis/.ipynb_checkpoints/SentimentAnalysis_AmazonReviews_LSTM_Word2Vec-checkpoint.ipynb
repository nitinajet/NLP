{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOSteaPMioXX"
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oneadmin\\OneDrive\\Documents\\LSTM\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\oneadmin\\OneDrive\\Documents\\LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cCFgZNHioXZ"
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "iIxQuR9QioXb",
    "outputId": "4cd3534c-33cc-4668-de44-929d9d490bb0"
   },
   "outputs": [],
   "source": [
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 80000 #We're training on the first 80,000 reviews in the dataset\n",
    "num_test = 20000 #Using 20,000 reviews from test set\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzrPmSgUioXl"
   },
   "outputs": [],
   "source": [
    "train_samples = 80000\n",
    "test_samples = 20000\n",
    "#validation_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "o4OGKNaeioXn",
    "outputId": "3136c833-c934-4861-af2a-7bd828ffeff6"
   },
   "outputs": [],
   "source": [
    "train_data = train_file[:train_samples]\n",
    "test_data = test_file[:test_samples]\n",
    "#validation_data = data[:validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVQIiLg2ioXo"
   },
   "source": [
    "##### Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKdVm3QfioXp"
   },
   "outputs": [],
   "source": [
    "def data_label_sep(data):\n",
    "    data_sentences = []\n",
    "    data_labels = []\n",
    "    for i in range(len(data)):\n",
    "        data_sentences.append(data[i].split(' ',1)[1].lower())\n",
    "        if data[i].split()[0]=='__label__1':\n",
    "            data_labels.append(0) #negative\n",
    "        else:\n",
    "            data_labels.append(1) #positive\n",
    "    return data_sentences,data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "c_6kafPAioXr",
    "outputId": "3d9b8052-92aa-43e0-ebda-11488f2b18f9"
   },
   "outputs": [],
   "source": [
    "train_sentences,train_labels = data_label_sep(train_data)\n",
    "test_sentences,test_labels = data_label_sep(test_data)\n",
    "#validation_sentences,validation_labels = data_label_sep(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-0fYM4rioXt"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaZFZ9oBioXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oneadmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bi1WbyUrioXt"
   },
   "outputs": [],
   "source": [
    "# Modify URLs to <url>\n",
    "def remove_urls(data):\n",
    "    for i in range(len(data)):\n",
    "        if 'www.' in data[i] or 'http:' in data[i] or 'https:' in data[i] or '.com' in data[i]:\n",
    "            data[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"\", data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_G3KBh3_iEv"
   },
   "outputs": [],
   "source": [
    "#retain only characters in text\n",
    "def retain_only_characters(data):\n",
    "  for i,sentence in enumerate(data):\n",
    "    data[i] = re.sub('[^a-zA-Z\\s]','',sentence)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVK1N3oBioXx"
   },
   "source": [
    "#remove stopwords\n",
    "def remove_stopwords(data):\n",
    "  new_data = []\n",
    "  for sentence in data:\n",
    "    #print(sentence)\n",
    "    new_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "      if word not in stopwords:\n",
    "        #print(word)\n",
    "        new_sentence = new_sentence + word+' '\n",
    "        #print(new_sentence)\n",
    "    new_data.append(new_sentence.strip())\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "def remove_stopwords(data):\n",
    "  new_data = []\n",
    "  for sentence in data:\n",
    "    #print(sentence)\n",
    "    new_sentence = []\n",
    "    for word in sentence.split():\n",
    "      if word not in stopwords:\n",
    "        #print(word)\n",
    "        new_sentence.append(word)\n",
    "        #print(new_sentence)\n",
    "    new_data.append(new_sentence)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "As3_LGaGDcQi"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "  data = remove_urls(data)\n",
    "  data = retain_only_characters(data)\n",
    "  data = remove_stopwords(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IAjq-rlJCBSm",
    "outputId": "65cc21e0-4692-4814-d620-bfdeec9efee6"
   },
   "outputs": [],
   "source": [
    "data = ['my name is nithin45 teja. @@this $$is my%% home!!34','jh bbeifu ibbi Nithin Teja']\n",
    "data = preprocess_data(data)\n",
    "docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
    "mod = Doc2Vec(docs, vector_size=50, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2238427e-03, -4.2259744e-03, -2.7536470e-04,  2.1858474e-03,\n",
       "        3.2720368e-03, -4.5986930e-03,  2.1028388e-03,  7.6276176e-03,\n",
       "        5.4585934e-03, -8.8823913e-04, -8.9581631e-04, -4.4225436e-03,\n",
       "       -8.6819334e-03,  3.1122388e-03,  1.0666648e-03, -2.6876648e-04,\n",
       "        9.3811573e-03, -4.7929692e-03, -1.3789731e-03,  8.0248695e-03,\n",
       "        6.6549368e-03,  2.7294592e-03,  5.0619682e-03,  8.6652683e-03,\n",
       "        4.6838578e-03, -2.9825841e-03, -3.6134545e-03,  2.7910073e-03,\n",
       "        8.1091141e-03,  7.1234845e-06,  5.5063097e-03,  7.0102676e-03,\n",
       "        7.4834111e-03,  6.5160817e-03,  8.5553052e-03,  8.2869912e-03,\n",
       "       -5.1968815e-03, -9.5076999e-03, -6.9928740e-04, -7.1212491e-03,\n",
       "        2.0153227e-03,  6.3076289e-03, -3.5129162e-03, -3.1343652e-03,\n",
       "       -9.0189967e-03, -9.8766712e-03, -9.3437573e-03,  2.7396013e-03,\n",
       "        1.7459539e-03, -3.7541887e-04], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.infer_vector(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "LErtoI_0ioXy",
    "outputId": "349455ea-6c36-4172-e2d5-81f574ac96ed"
   },
   "outputs": [],
   "source": [
    "train_sentences = preprocess_data(train_sentences)\n",
    "test_sentences = preprocess_data(test_sentences)\n",
    "#validation_sentences = preprocess_data(validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwUkHkFRioX6"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivRF-8SfioX9"
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_sentences)]\n",
    "doc_model = Doc2Vec(documents, vector_size=50, window=2, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_vectors(data):\n",
    "    data_vectors = []\n",
    "    for sent in data:\n",
    "        data_vectors.append(doc_model.infer_vector(sent))\n",
    "    return data_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_vectors = get_sent_vectors(train_sentences)\n",
    "test_sent_vectors = get_sent_vectors(test_sentences)\n",
    "#validation_sent_vectors = get_sent_vectors(validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_vectors = np.array(train_sent_vectors)\n",
    "test_sent_vectors = np.array(test_sent_vectors)\n",
    "#validation_sent_vectors = np.array(validation_sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "#validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5\n",
    "split_id = int(split_frac * len(test_sent_vectors))\n",
    "val_sent_vectors, test_sent_vectors = test_sent_vectors[:split_id], test_sent_vectors[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18634628, -0.23145925,  0.22732793,  0.01240477, -0.09934179,\n",
       "       -0.02743756, -0.17262034, -0.01977576, -0.05377217,  0.25906742,\n",
       "        0.21284275,  0.05942927,  0.13956027, -0.06918056,  0.22931278,\n",
       "       -0.2188386 , -0.13801558,  0.02782463, -0.05958407, -0.28977776,\n",
       "       -0.1518366 , -0.00568582, -0.03498911, -0.18451211, -0.03240382,\n",
       "        0.1163968 ,  0.09131456, -0.06188005, -0.17871764, -0.05976041,\n",
       "        0.01440505, -0.26516125,  0.04809568, -0.29617298, -0.14828905,\n",
       "        0.23835751,  0.05231535,  0.05524227, -0.00805352, -0.16551611,\n",
       "        0.2862892 , -0.00275837,  0.38594207, -0.05428948,  0.00939994,\n",
       "       -0.23627755,  0.16189641, -0.06248271,  0.00087817,  0.02896786],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sent_vectors), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sent_vectors), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sent_vectors), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1863, -0.2315,  0.2273,  0.0124, -0.0993, -0.0274, -0.1726, -0.0198,\n",
       "         -0.0538,  0.2591,  0.2128,  0.0594,  0.1396, -0.0692,  0.2293, -0.2188,\n",
       "         -0.1380,  0.0278, -0.0596, -0.2898, -0.1518, -0.0057, -0.0350, -0.1845,\n",
       "         -0.0324,  0.1164,  0.0913, -0.0619, -0.1787, -0.0598,  0.0144, -0.2652,\n",
       "          0.0481, -0.2962, -0.1483,  0.2384,  0.0523,  0.0552, -0.0081, -0.1655,\n",
       "          0.2863, -0.0028,  0.3859, -0.0543,  0.0094, -0.2363,  0.1619, -0.0625,\n",
       "          0.0009,  0.0290]), tensor(0, dtype=torch.int32))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55585\n"
     ]
    }
   ],
   "source": [
    "vocab = len(doc_model.wv.vocab)\n",
    "#vocab = 14868\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): Embedding(55586, 50)\n",
      "  (lstm): LSTM(50, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab + 1\n",
    "output_size = 1\n",
    "embedding_dim = 50\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #print(inputs)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SentimentAnalysis-AmazonReviews-LSTM-Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
